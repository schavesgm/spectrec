# Import built-in modules
import os, random, re, builtins

# Import third-party modules
import torch
import torch.distributed as dist
import torch.backends.cudnn as cudnn

def disable_print_for_non_main(rank: int):
    """ Disable printing for non-master (rank=0) processes """
    builtin_print = builtins.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if (rank == 0) or force:
            builtin_print(*args, **kwargs)
    
    builtins.print = print

def fix_random_seeds(seed: int = 31):
    """ Fix the random seeds for torch random numbers. 

    --- Parameters:
    seed: int
        Seed to be used in the program.
    """

    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def initialise_dist_nodes(gpu_devices: str, num_nodes: int, node: int, workers: int) -> dict:
    """ Initialise the distributed nodes using several gpu_devices and workers. The
    GPU devices are useful to declare the visible CUDA devices in the computation and
    they must be on the form, '0, 1'...

    --- Parameters:
    gpu_devices: str
        GPU devices used for each node.
    num_nodes: int
        Number of nodes used in the calculation.
    node: int
        Current node used in the calculation.
    workers: int
        Processes used when loading the data.

    --- Returns:
    dict:
        Dictionary containing relevant node information.
    """

    # Assert some conditions on the nodes
    assert num_nodes >= 1, f'{num_nodes=} must be at least 1.'
    assert 0 <= node < num_nodes, f'{node=} must be in [0,{num_nodes})'

    # Assert some conditions on the gpu devices string
    assert all([re.match(r'\d+', d) for d in gpu_devices.split(',')]), \
        f'{gpu_devices=} must have the form 0,1,2,3,...,Nd'

    # Generate the port used in the distributed training
    port = random.randint(49152, 65535)

    # Set the all visible cuda devices
    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_devices

    return {
        'rank':          0,
        'gpus_per_node': torch.cuda.device_count(),
        'world_size':    torch.cuda.device_count() * num_nodes,
        'num_nodes':     num_nodes,
        'node':          node,
        'url':           f'tcp://localhost:{port}',
        'workers':       workers,
        'devices':       [int(d) for d in gpu_devices.split(',')]
    }

def initiliase_dist_gpu(gpu: int, node_info: dict) -> int:
    """ Initialise the distributed system for the current gpu. 

    --- Parameters:
    gpu: int
        Gpu device number.
    node_info: dict
        Dictionary containing meaningful information. Generated by initialise_dist_nodes
    
    --- Returns
    int:
        Global rank of the current GPU.
    """

    # Generate the rank of the current gpu
    rank = node_info['node'] * node_info['world_size'] + gpu

    # Initialise the process group of this gpu
    dist.init_process_group(
        backend='nccl', init_method=node_info['url'], 
        world_size=node_info['world_size'], rank=rank
    )

    # Fix all random seeds for this device
    fix_random_seeds()

    # Set some properties in the backend
    cudnn.benchmark = True
    cudnn.enabled   = True

    # Synchronise all devices
    dist.barrier(device_ids=node_info['devices'])

    # Disable printing for non master ranks
    disable_print_for_non_main(rank)

    return rank

if __name__ == '__main__':
    pass
